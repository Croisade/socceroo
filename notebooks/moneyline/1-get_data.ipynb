{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import sys\n",
    "from random import randint\n",
    "from time import sleep\n",
    "sys.path.append(\"../..\")\n",
    "from config.settings import get_data_url, OddsMatchColumn, ODDS_COLUMN_VALUES, get_data_directory_matches,DIRECTORY_MLS_MATCHES_ODDS, TEAM_NAME_TO_CLUB, DIRECTORY_COMBINED_MATCHES_CLEAN, extract_number, Club, RELEVANT_YEARS, DIRECTORY_COMBINED_MATCHES_CLEAN_AGG, LINEAR_COLS, POINTS_MAP, MatchLogTypes\n",
    "pd.set_option('display.max_columns', 70)\n",
    "df = pd.read_csv(\"../../data/Combined_Team_Stats.csv\")\n",
    "df_matches = pd.read_csv(\"../../data/Combined_MLS_Data2024.csv\")\n",
    "df_odds = pd.read_excel(\"../../data/mls_historical_odds.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_with_multiple_formats(date_str):\n",
    "    for fmt in ('%m/%d/%Y', '%Y-%m-%d %H:%M:%S'):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError as e:\n",
    "            print(f\"{date_str} , Error occurred: {e}\")\n",
    "            continue\n",
    "    return pd.NaT  # Return Not a Time for unparseable formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_team_names(df: pd.DataFrame, columns: list[str]) -> pd.DataFrame:\n",
    "    for col in columns:\n",
    "        df[col] = df[col].map(lambda x: TEAM_NAME_TO_CLUB.get(x, None).value if x in TEAM_NAME_TO_CLUB else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamal\\AppData\\Local\\Temp\\ipykernel_24256\\811614169.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Date'] = filtered_df['Date'].apply(parse_date_with_multiple_formats)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_df = df_odds[~df_odds['Season'].between(2012, 2017)]\n",
    "filtered_df['Date'] = filtered_df['Date'].apply(parse_date_with_multiple_formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nashville-SC' 'Atlanta-United' 'Charlotte-FC' 'DC-United'\n",
      " 'FC-Cincinnati' 'Inter-Miami' 'Orlando-City' 'Philadelphia-Union'\n",
      " 'Austin-FC' 'FC-Dallas' 'Vancouver-Whitecaps-FC' 'Seattle-Sounders'\n",
      " 'Portland-Timbers' 'Columbus-Crew' 'New-England-Revolution'\n",
      " 'New-York-Red-Bulls' 'Chicago-Fire' 'St-Louis-City' 'Colorado-Rapids'\n",
      " 'San-Jose-Earthquakes' 'Los-Angeles-FC' 'LA-Galaxy' 'Houston-Dynamo'\n",
      " 'New-York-City-FC' 'Toronto-FC' 'Minnesota-United' 'Real-Salt-Lake'\n",
      " 'Sporting-Kansas-City' 'CF-Montreal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamal\\AppData\\Local\\Temp\\ipykernel_24256\\242616791.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].map(lambda x: TEAM_NAME_TO_CLUB.get(x, None).value if x in TEAM_NAME_TO_CLUB else x)\n"
     ]
    }
   ],
   "source": [
    "df_odds_clean = filtered_df[ODDS_COLUMN_VALUES]\n",
    "df_odds_clean = normalize_team_names(filtered_df, [OddsMatchColumn.HOME_TEAM.value, OddsMatchColumn.AWAY_TEAM.value])\n",
    "print(df_odds_clean[OddsMatchColumn.HOME_TEAM.value].unique())\n",
    "df_odds_clean.to_csv(DIRECTORY_MLS_MATCHES_ODDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [\n",
    "        col[1].strip() if isinstance(col, tuple) and len(col) > 1 else str(col).strip()\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def normalize_date_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if \"date\" in col.lower():\n",
    "            df.rename(columns={col: \"Date\"}, inplace=True)\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors='coerce')\n",
    "            break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fbref_table(url: str, ATTRs: dict = {\"id\": \"matchlogs_for\"}) -> pd.DataFrame:\n",
    "    try:\n",
    "        tables = pd.read_html(url, ATTRs=ATTRs)\n",
    "        df = tables[0]\n",
    "        df = flatten_columns(df)\n",
    "        df = normalize_date_column(df)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading table from {url}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fbref_matchlogs(club: Club, year: int, log_types: list[MatchLogTypes]) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for log_type in log_types:\n",
    "        sleep(randint(15,20))\n",
    "        url = get_data_url(club, log_type.value, year)\n",
    "        df = load_fbref_table(url)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on=\"Date\", how=\"outer\"), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading table from https://fbref.com/en/squads/eb57545a/2021/matchlogs/all_comps/shooting/Charlotte-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/eb57545a/2021/matchlogs/all_comps/gca/Charlotte-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/91b092e1/2021/matchlogs/all_comps/shooting/San-Diego-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/91b092e1/2021/matchlogs/all_comps/gca/San-Diego-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/bd97ac1f/2021/matchlogs/all_comps/shooting/St-Louis-City-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/bd97ac1f/2021/matchlogs/all_comps/gca/St-Louis-City-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/91b092e1/2022/matchlogs/all_comps/shooting/San-Diego-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/91b092e1/2022/matchlogs/all_comps/gca/San-Diego-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/bd97ac1f/2022/matchlogs/all_comps/shooting/St-Louis-City-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/bd97ac1f/2022/matchlogs/all_comps/gca/St-Louis-City-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/91b092e1/2023/matchlogs/all_comps/shooting/San-Diego-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/91b092e1/2023/matchlogs/all_comps/gca/San-Diego-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/91b092e1/2024/matchlogs/all_comps/shooting/San-Diego-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/91b092e1/2024/matchlogs/all_comps/gca/San-Diego-FC-Match-Logs-All-Competitions: HTTP Error 500: Internal Server Error\n",
      "Error loading table from https://fbref.com/en/squads/130f43fa/2025/matchlogs/all_comps/gca/Toronto-FC-Match-Logs-All-Competitions: HTTP Error 502: Bad Gateway\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidIndexError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m         merged_df[\u001b[33m'\u001b[39m\u001b[33mTeam\u001b[39m\u001b[33m'\u001b[39m] = c.value\n\u001b[32m     12\u001b[39m         df_list.append(merged_df)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m combined_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m combined_df.to_csv(get_data_directory_matches(year))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\util\\_decorators.py:311\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    306\u001b[39m     warnings.warn(\n\u001b[32m    307\u001b[39m         msg.format(arguments=arguments),\n\u001b[32m    308\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    309\u001b[39m         stacklevel=stacklevel,\n\u001b[32m    310\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:307\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03mConcatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03malong the other axes.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    292\u001b[39m \u001b[33;03mValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m op = _Concatenator(\n\u001b[32m    295\u001b[39m     objs,\n\u001b[32m    296\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    304\u001b[39m     sort=sort,\n\u001b[32m    305\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:528\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    526\u001b[39m         obj_labels = obj.axes[\u001b[32m1\u001b[39m - ax]\n\u001b[32m    527\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels.equals(obj_labels):\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m             indexers[ax] = \u001b[43mobj_labels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    530\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m    532\u001b[39m new_data = concatenate_managers(\n\u001b[32m    533\u001b[39m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m.new_axes, concat_axis=\u001b[38;5;28mself\u001b[39m.bm_axis, copy=\u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m    534\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3442\u001b[39m, in \u001b[36mIndex.get_indexer\u001b[39m\u001b[34m(self, target, method, limit, tolerance)\u001b[39m\n\u001b[32m   3439\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_method(method, limit, tolerance)\n\u001b[32m   3441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_as_unique:\n\u001b[32m-> \u001b[39m\u001b[32m3442\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m._requires_unique_msg)\n\u001b[32m   3444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_compare(target) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interval_dtype(\u001b[38;5;28mself\u001b[39m.dtype):\n\u001b[32m   3445\u001b[39m     \u001b[38;5;66;03m# IntervalIndex get special treatment bc numeric scalars can be\u001b[39;00m\n\u001b[32m   3446\u001b[39m     \u001b[38;5;66;03m#  matched to Interval scalars\u001b[39;00m\n\u001b[32m   3447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_indexer_non_comparable(target, method=method, unique=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mInvalidIndexError\u001b[39m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "for year in RELEVANT_YEARS:\n",
    "    df_list = []\n",
    "    for c in Club:\n",
    "        merged_df = merge_fbref_matchlogs(\n",
    "            c,\n",
    "            year,\n",
    "            [MatchLogTypes.Shooting, MatchLogTypes.GoalAndShotCreation]\n",
    "        )\n",
    "        merged_df['Team'] = c.value\n",
    "        if not merged_df.empty:\n",
    "            merged_df['Team'] = c.value\n",
    "            df_list.append(merged_df)\n",
    "\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    combined_df.to_csv(get_data_directory_matches(year))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def agg_relevant_stats(year, df):\n",
    "    # Map points for all rows first\n",
    "    df['Pts'] = df['Result_x'].map(POINTS_MAP)\n",
    "    df['GF_x'] = df['GF_x'].apply(extract_number)\n",
    "\n",
    "    # Set points to NaN or 0 where it's not Regular Season\n",
    "    df.loc[df['Round_x'] != 'Regular Season', 'Pts'] = np.nan  # or `np.nan` if you want to ignore\n",
    "\n",
    "    summary_df = df.groupby(\"Team\")[LINEAR_COLS + ['Pts']].sum().reset_index()\n",
    "    summary_df['Season'] = year\n",
    "    summary_df = summary_df.rename(columns={\n",
    "        \"Result_x\": \"Result\",\n",
    "        \"Round_x\": \"Round\",\n",
    "        \"GF_x\": \"GF\",\n",
    "        \"Sh_x\": \"SH\"\n",
    "    })\n",
    "    return summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_stats(df):\n",
    "    # Desired columns\n",
    "    desired_cols = LINEAR_COLS + ['Team', 'Date', 'Season']\n",
    "    # Only keep those that actually exist\n",
    "    available_cols = [col for col in desired_cols if col in df.columns]\n",
    "\n",
    "    # Now safely select\n",
    "    df = df[available_cols]\n",
    "    df= df.rename(columns={\n",
    "            \"Result_x\": \"Result\",\n",
    "            \"Round_x\": \"Round\",\n",
    "            \"GF_x\": \"GF\",\n",
    "            \"Sh_x\": \"SH\"\n",
    "        })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_list_agg_clean = []\n",
    "df_combined_list = []\n",
    "for year in RELEVANT_YEARS:\n",
    "    if year == 2025:\n",
    "        continue\n",
    "    df = pd.read_csv(get_data_directory_matches(year))\n",
    "    df[\"Season\"] = year\n",
    "\n",
    "    df_clean = get_relevant_stats(df.copy())\n",
    "    df_combined_list.append(df_clean)\n",
    "\n",
    "    df_agg_clean = agg_relevant_stats(year, df.copy())\n",
    "    df_combined_list_agg_clean.append(df_agg_clean)\n",
    "\n",
    "\n",
    "df_combined_clean = pd.concat(df_combined_list_agg_clean)\n",
    "df_combined_clean.to_csv(DIRECTORY_COMBINED_MATCHES_CLEAN_AGG)\n",
    "\n",
    "df_combined = pd.concat(df_combined_list)\n",
    "df_combined.to_csv(DIRECTORY_COMBINED_MATCHES_CLEAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
